---
title: "32061412_Assignment2"
author: "HongYi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
editor_options: 
  markdown: 
    wrap: sentence
---

## Libraries loading

```{r}
library(tidyverse)
library(lubridate)
library(stringr)
library(tm)
library(tokenizers)
library(wordcloud2)
```

## Data exploration

```{r}
# read the news data
news <- read_csv("ireland_news.csv")
```

```{r}
head(news)
```

```{r}
glimpse(news)
```

## Question 1

What are the earliest and latest articles from Irish Independent, irrespective of headline category?
Please also sort the data according to the column publish_date in an ascending manner and display the last 5 records of the data.

### Answer

-   only publish_date and headline category are the selected columns because we already know the data is from Irish Independent and headline category is irrelevant

-   before sorting, filter out the NA publish_date column

-   arrange the data based on the publish_date in date type

-   show earliest and latest using head() and tail()

```{r}
# get Irish Independent's articles only
news_irish <- news %>%
  filter(news_provider == "Irish Independent") %>%
  filter(!is.na(publish_date)) %>%
  mutate(publish_date = dmy(publish_date)) %>%
  select(publish_date, headline_text) %>%
  arrange(publish_date)

# show earliest
head(news_irish,1)

# show latest
tail(news_irish,1)
```

```{r}
# filter out NA date, change the data type of date, and sort asc based on date
sorted_news <- news %>%
  filter(!is.na(publish_date)) %>%
  mutate(publish_date = dmy(publish_date)) %>%
  arrange(publish_date)
  
# view data in ascending order 
sorted_news

# display last 5 records(5 latest data)
tail(sorted_news, 5)
```

## Question 2

How many unique headline_category values are there in the data file?
Please consider variations (e.g.: capitalisation, potential inconsistencies) of the headline_category values, when counting them.

How many news category articles contain either the keyword, "Ireland", "Irish", "US", or "USA" along with year digits from 2000 to 2024 in headline_text?
For example, you need to search and count articles containing both “Ireland” and the year digits, or containing both “Irish” and the year digits, and so on.

### Answer

During data exploration, headline category separated by "\_" instead of "." is found and contains NA data.

```{r}
capital_exist <- news %>% 
  filter(str_detect(headline_category, "[A-Z]+"))

underscore_exist <- news %>%
  filter(str_detect(headline_category, "_")) 

na_exist <- news %>%
  filter(is.na(headline_category))

capital_exist
underscore_exist
na_exist
```

Therefore, do counting after replacing "\_" with "." , lower case them and filter out the NA headline_category.

```{r}

uniq_category <- news %>%
  # transform headline_category column to lower case
  mutate(headline_category = tolower(headline_category)) %>%
  # replace all underscores to dots in headline_category column
  mutate(headline_category = str_replace_all(headline_category,"_",".")) %>%
  # filter na data
  filter(!is.na(headline_category)) %>%
  # get unique rows only
  distinct(headline_category)

nrow(uniq_category)
```

Filter rows that are from "news" category and contain "Ireland", "Irish", "US", or "USA" and 2000 to 2024 in headline_text using pure text comparison

```{r}
news_categories <- news %>%
  filter(headline_category == "news") %>%
  filter(str_detect(headline_text, "US|USA|Irish|Ireland")) %>%
  filter(str_detect(headline_text, "2000|2001|2002|2003|2004|2005|2006|2007|2008|2009|2010|2011|2012|2013|2014|2015|2016|2017|2018|2019|2020|2021"))

nrow(news_categories)
```

## Question 3

Please display the top 10 headline categories with the largest number of articles published on Monday throughout the years.
Then, draw a chart showing the total number of articles for the top 10 headline categories (as identified previously) for each year.
What can you observe?
Please discuss the chart and your findings.

### Answer

-   filter such that it is Monday, non-NA(like Q2),

-   transform headline_category, lower case and inconsistency such as "\_" (like Q2)

-   group by headline_category

-   calculate count for each group

-   descending order based on count

```{r}
top_10 <- news %>%
  # Monday only
  filter(str_detect(publish_date, "Monday")) %>%
  # filter NA headline category
  filter(!is.na(headline_category)) %>%
  # transform headline_category column to lower case
  mutate(headline_category = tolower(headline_category)) %>%
  # replace all underscores to dots in headline_category column
  mutate(headline_category = str_replace_all(headline_category,"_",".")) %>%
  # group by headline category to get the count for each group
  group_by(headline_category) %>%
  # display headline_category with respective count
  summarise(count = n_distinct(headline_text, na.rm = TRUE)) %>%
  ungroup() %>%
  # arrange in desc order
  arrange(desc(count)) %>%
  head(10)

top_10
```

For second part of the Question 3, additionally,

-   filter such that it has date and not NA
-   filter such that it is from the top 10 headline_category above
-   transform date to year into another column
-   group by both headline_category and year

```{r}
top_10_over_years <- news %>%
  # filter so that date is not unavailable
  filter(!is.na(publish_date)) %>%
  # filter the top 10 headline_category
  filter(headline_category %in% top_10$headline_category) %>%
  mutate(headline_category = tolower(headline_category)) %>%
  mutate(headline_category = str_replace_all(headline_category,"_",".")) %>%
  # change to Date type
  mutate(publish_date = dmy(publish_date)) %>%
  # get year from Date and put into 'year' column
  mutate(year = format(publish_date, "%Y")) %>%
  # additionally, group by year
  group_by(headline_category, year) %>%
  summarise(count = n_distinct(headline_text, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(desc(count))

# show top 10 each year
top_10_over_years
```

Plot line graph

-   The x and y axis are flipped because if x-axis is 'Year', the range of the 'Year' is too large and causes the number to be overlapped and hard to see

```{r}
# plot 
top_10_over_years %>%
  ggplot(aes(x = year, y = count, group = headline_category, color=headline_category)) +
  geom_line(size=1) +
  xlab("Year") +
  ylab("Count") +
  labs(title="Total number of articles for the top 10 headline categories for each year", subtitle = "News has much larger number of articles than other categories before 2013") + 
  coord_flip()
```

#### Findings

-   It can be seen as 3 levels number of articles

    1.  News has much larger number of articles than others before 2013
    2.  Sport and Business maintained roughly between 5000 to 10000 articles before 2010(inclusive) and before 2012(inclusive) respectively
    3.  All the other headline categories have below 5000 articles over the years

-   There are 3 obvious lines that only have articles since 1999(sport.rugby, sport.soccer) and 2008(news.law)

## Question 4

Compute the total number of articles for each headline category and news provider.
Then, use a single R function/command to display the statistical information, i.e., Min, Max, and Mean, of the total number of articles (as computed previously) for each news provider.
Note: You can use multiple functions/commands to get the desired pre-processed data table, but when you compute and display the statistical information, you need to use a single R function/command.

### Answer

-   "for each headline category and news provider" indicates group by headline_category and news_provider

-   calculate count for each group and put result into a new column

```{r}
articles_cate_prov <- news %>%
  # filter out NA category and provider
  filter(!is.na(headline_category) & !is.na(news_provider)) %>%
  # for each headline category and news provider
  group_by(headline_category, news_provider) %>%
  # calculate count
  summarise(count = n_distinct(headline_text, na.rm = TRUE)) %>%
  ungroup() 

# show data in desc order
articles_cate_prov %>% arrange(desc(count))
```

-   use aggregate() function to cast a summary() onto each news provider group

```{r}
# display statistical info using aggregate() and pass in summary
summary_data <- aggregate(articles_cate_prov$count, list(articles_cate_prov$news_provider), summary)
# cant be displayed
summary_data
```

```{r}
# parse it into dataframe so it can be displayed
summary_data <- do.call(data.frame, summary_data)
summary_data
```

## Question 5

Please compute the total number of articles for each headline category, news provider, and the day of the week.
Then, compute the average number of articles for each news provider and the day of the week, based on the total number of articles computed previously.
After that, please display the day of the week with the highest average number of articles for each provider.
The output data should be structured in the following format.

### Answer

-   filter out NA category, publish date and provider

-   get weekday label by using wday onto date-converted(dym() function) publish_date

-   for each headline category and news provider(group by), calculate count(n_distinct)

    -   n_distinct is used here because same article might be published more than once

-   display in descending order based on count

```{r}

articles_cate_prov_day <- news %>%
  # filter out NA category, publish date and provider
  filter(!is.na(headline_category) & 
           !is.na(news_provider) & 
           !is.na(publish_date)) %>%
  # get weekday label by using wday onto date-converted publish_date
  mutate(weekday = wday(dmy(publish_date), label=TRUE)) %>%
  # for each headline category and news provider
  group_by(headline_category, news_provider, weekday) %>%
  # calculate count
  summarise(count = n_distinct(headline_text, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(desc(count))
  
articles_cate_prov_day 
```

"Then, compute the average number of articles for each news provider and the day of the week, based on the total number of articles computed previously" indicates to compute average across categories for each news provider and day of the week

-   group by news provider and day of the week

-   sum up the count calculated in previous ques, get number of articles and calculate average using sum of count/number of articles

```{r}
articles_cate_prov_day <- articles_cate_prov_day %>%
  group_by(news_provider, weekday) %>%
  summarise(total_articles = sum(count, na.rm = TRUE),
            total_categories = n(),
            average = total_articles/total_categories) %>%
  ungroup() %>%
  arrange(desc(average)) %>%
  select(news_provider, weekday, average)
```

-   use pure string to set up the column names

-   group by provider

-   filter out the average is not the max among the days in the week

-   floor the average to get integer of the average

```{r}
articles_cate_prov_day %>%
  group_by(news_provider) %>%
  filter(average == max(average, na.rm=TRUE)) %>%
  summarise("News provider"= news_provider,
            "The day of week (with the highest
average number of articles)" = weekday,
           "The highest average
number of articles"= floor(average)) %>%
  ungroup() %>%
  select("News provider", "The day of week (with the highest
average number of articles)", "The highest average
number of articles")
```

## Question 6

### Answer

-   filter outNA publish date

-   change publish date to date type so that can do comparison

-   do comparison to get 2019 and 2020 data

-   add Period column values by doing comparison as well

```{r}
add_period <- news %>%
  filter(!is.na(publish_date)) %>%
  mutate(date = dmy(publish_date)) %>%
  filter(date >= '2019-01-01' & date <= '2020-12-31') %>%
  mutate(Period = case_when(date >='2019-01-01'& date<='2019-03-31' ~ "Period 1",
                            date >='2019-04-01'& date<='2019-06-30' ~ "Period 2",
                            date >='2019-07-01'& date<='2019-09-30' ~ "Period 3",
                            date >='2019-10-01'& date<='2019-12-31' ~ "Period 4",
                            date >='2020-01-01'& date<='2020-03-31' ~ "Period 5",
                            date >='2020-04-01'& date<='2020-06-30' ~ "Period 6",
                            date >='2020-07-01'& date<='2020-09-30' ~ "Period 7",
                            date >='2020-10-01'& date<='2020-12-31' ~ "Period 8",))
  
add_period %>% select(publish_date, date, Period)
```

-   filter out NA headline_category

-   filter such that only top_10 computed in Question 3 are included

-   resolve inconsistencies in headline_category similar to Question 2

-   "by period and headline category" indicates group by both Period and headline_category

-   calculate number of articles using n_distinct()

```{r}
top10_during_periods <- add_period %>%
  # filter na data
  filter(!is.na(headline_category)) %>%
  # filter the top 10 headline_category
  filter(headline_category %in% top_10$headline_category) %>%
  # lowercase the headline category because inconsistencies
  mutate(headline_category = tolower(headline_category)) %>%
  # replace all underscores to dots in headline_category column
  mutate(headline_category = str_replace_all(headline_category,"_",".")) %>%
  group_by(headline_category, Period) %>%
  summarise(total_articles = n_distinct(headline_text)) %>%
  ungroup() %>%
  arrange(desc(total_articles))

top10_during_periods 
```

-   plot boxplot with jitter

```{r}
top10_during_periods %>%
  ggplot(aes(x = Period, y = total_articles, group = Period)) +
  geom_jitter(alpha = 0.3) +
  geom_boxplot() +
  ylab("Number of articles") +
  labs(title="Number of articles from the top 10 headline categories for each period")
```

## Question 7

Please sample 1% of the data, conduct the text pre-processing for the values of the headline_text column in the sampled data, and display a portion (the first few columns and rows) of a document-term matrix generated.
Then, draw a plot showing the top 10 most frequent words where the x-axis represents the frequency of words and the y-axis represents the words themselves.
Additionally, generate a word cloud.

### Answer

-   Tokenization on every row

```{r}
# set seed for random sample
set.seed(32061412)

# 1% of the data
one_percent_sample <- news %>%
  sample_frac(0.01) 

# apply tokenization for each row
tokenised_sample <- 
  lapply(one_percent_sample$headline_text, function(line) {
    unlist(tokenize_words(line))
  })

# check the data
tokenised_sample[1]
```

-   Create Corpus object that provides structured and efficient framework for text analysis

```{r}
one_percent_sample$doc_id <- seq(nrow(one_percent_sample))
one_percent_sample <- one_percent_sample %>%
  select(headline_text, doc_id)
  
names(one_percent_sample)[names(one_percent_sample) == 'headline_text'] <- 'text'
one_percent_sample
```

```{r}
# Create your DataFrameSource
sample_source <- DataframeSource(one_percent_sample)

# Create a Corpus
sample_corpus <- Corpus(sample_source)

# Check corpus
sample_corpus
```

-   Remove stop words, punctuation, numbers and spaces and case normalisation

```{r}
# remove stop words
sample_corpus <- tm_map(sample_corpus, removeWords, stopwords("en")) 

# remove punctuation
sample_corpus <- tm_map(sample_corpus, removePunctuation) 

# remove all numbers
sample_corpus <- tm_map(sample_corpus, removeNumbers)

# remove redundant spaces
sample_corpus <- tm_map(sample_corpus, stripWhitespace) 

# case normalisation
sample_corpus <- tm_map(sample_corpus, content_transformer(tolower))
```

-   Stemming

```{r}
# perform stemming to reduce inflected and derived words to their root form
sample_stem <- tm_map(sample_corpus, stemDocument) 

# Inspect the stemmed corpus
# inspect(sample_stem[1])
```

-   Create document-term matrix

```{r}
#  Create a matrix which its rows are the documents and columns are the words. 
sample_dtm <- DocumentTermMatrix(sample_stem)

# check dtm
inspect(sample_dtm)
```

-   Plot top 10 used words and their frequencies

```{r}
# Convert the DocumentTermMatrix into a regular matrix object and calculate term frequencies
term_freq<- colSums(as.matrix(sample_dtm))

# Create a dataframe
df<- data.frame(term = names(term_freq), freq = term_freq)

# Filter terms with a frequency of at least 100
df <- df %>%
  filter(freq>=100) %>%
  arrange(desc(freq))

# Select the top 10 frequent words
df_plot<- df %>%
  top_n(10, freq)

# Plot word frequency
ggplot(df_plot, aes(x = fct_reorder(term, freq), y = freq, fill = freq)) + 
  geom_bar(stat = "identity") + 
  xlab("Terms") + 
  ylab("Count") + 
  coord_flip()

```

Wordcloud

```{r}
wordcloud2(df, color = "random-dark", backgroundColor = "white")
```

## Question 8

### Answer

```{r}
irish_times_performance <- news %>%
  filter(!is.na(publish_date)) %>%
  mutate(date = dmy(publish_date)) %>%
  # filter(date >= '2015-01-01' & date <= '2015-12-31') %>%
  mutate(Year = year(date)) %>%
  mutate(Week = week(date)) %>%
  filter(news_provider == "Irish Times") %>%
  group_by(Year ,Week) %>%
  summarise(count = n_distinct(headline_text, na.rm = TRUE)) %>%
  ungroup()

head(irish_times_performance)
```

```{r}
irish_times_performance %>%
  ggplot(aes(x = Week, y = count, group = Year, color=(Year==2020))) +
  geom_line() +
  xlab("Week") +
  ylab("Number of articles published") +
  labs(title="Irish Times's performance from 1996 to 2021") 
```

#### Discussion

-   If news provider's performance is based on number of articles within certain time frame then plotting a graph of number of articles vs specific year can be used to observe the news provider's performance in that year.

-   The graph can also be used to monitor the performance of the provider company throughout the year so that downhill performance can be detected early, find out the existing problems within the company and provide corresponding solutions without further deterioration

-   Not only single year can be plotted, but also from the year that the company has started operation.
    By plotting the number of articles published across the year since the year of operation, the company can monitor whether itself has evolved and grown larger or even whether or not the company has been earning money.

-   The graph above shows that taking one year of performance and compare to performances in other years.
    From this graph, we can know that in 2020 the performance is deteriorating and almost lowest among all the other performances, corresponding actions and countermeasures can be come up with by the company directors to prevent their companies from further worsening.
